- time: 8:30 - 9:30
  info: "<b>Lanyards and Reception</b> <i>Seminar Room 23.21.00.44</i><br>Register with us and get your lanyards if you did not check in at the hostel. "
- time: 9:30 - 10:00
  info: "<b>Opening Plenary</b> <i>Lecture Hall 3F</i><br>Greeting and general information"
- time: 10:00 - 11:00
  info: "<b>Keynote talk by Dr. Yulia Zinova</b> <i>Lecture Hall 3F</i><br><details><summary>▼ Word Embeddings and Morphology</summary>\nClick <a href='/css/2023_style/TalkSlides/ZINOVA.pdf'>here</a> to download the slides. \n<br>In this talk I will present some of the ongoing research on morphology with static embeddings. I will talk about what kind of information gets implicitly learned, which limitations the models face and how linguistic knowledge can be used to boost performance of language models, especially for languages with less resources and/or rich morphology.</details>"
- time: 11:00-11:30
  info: "<b>Coffee Break</b> <i>Room 23.21.00.44</i>"
- time: 11:30-12:00
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.46</i><br><details><summary>▼ Throwing Shaders at Language Models - Evaluating Creative Code Generation - <i>Jan Kels</i></summary>\nClick <a href='/css/2023_style/TalkSlides/KELS.pdf'>here</a> to download the slides.<br>We introduce the Shadertoys dataset containing 27.508 shader programs gathered from Shadertoy.com, annotated with metadata. To evaluate how language models handle code generation we publish a benchmark suite for creative programming tasks. The first task serves as a proof of concept and evaluates models on their ability to generate the return statement of a function. Language models reached about 0.28 while code specific models reached 0.37 and fine tuned models managed 0.60 for said metric. The benchmark and associated dataset are available on Huggingface.</details><hr><b>Student talk</b> <i>Seminar Room 23.21.00.048</i><br><details><summary>▼ An Exploration of translanguaging on Social Media by Hispanic International Students in the United States - <i>Cristina Reguera-Gòmez</i></summary>[SLIDES]<br>International students from Spanish-Speaking countries in American universities often communicate in Spanish or English depending on the environment. However, they also interact in a way that can be described as the combination of both languages within the same discourse. This phenomena is known as translanguaging, and it is a common linguistic practice all around the world (de la Luz Reyes, 2012). It can be said that it involves the creation of new ways of communicating. Nevertheless, translanguaging is also often perceived as not being skilled in neither of the two languages (Wei, 2021). This social conception may cause students to restrain themselves from translanguaging with certain audiences, and reserve it for more casual conversations with fellow bilingual international students. The purpose of this research is to examine the translanguaging practices on social networks of Hispanic international students in American universities, with a focus on how they translanguage and what social functions their translanguaging practices have. To answer these questions, this study examines the posts and messages of five participants through critical and multilingual discourse analysis. The results show that participants translanguage at the sentential and suprasentential level, and that the type of translanguaging and its frequency are closely linked to the audience and the social network used. In other words, participants translanguage more frequently and in more diverse ways when communicating with fellow Spanish-English bilingual international students, while expressing different identities. The present study suggests that translanguaging is more complex than the mixture of two languages and its presence is highly reliant on the audience.</details>"  
- time: 12:10-12:40
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.46</i><br><details><summary>▼ Machine Transliteration between two Persian Dialects - The Case of Farsi and Tajiki - <i>Rayyan Merchant</i></summary>\nClick <a href='/css/2023_style/TalkSlides/MERCHANT.pptx'>here</a> to download the slides. \n<br>Despite speaking mutually intelligible varieties of the same language, speakers of Tajik Persian, written in a modified Cyrillic alphabet, cannot read Persian texts written in the Perso-Arabic script. Due to overwhelming similarity between the dialects, transliteration may be more appropriate than translation. Previous work created a statistical model, but lacked parallel corpora with which to judge the model (Davis, 2012). We aim to demonstrate that transliteration provides a way to “translate” between the two dialects, utilizing a neural-based approach to grapheme to phoneme conversion. Our work focuses on one direction: from the Perso-Arabic script to Cyrillic (henceforth referred to as Farsi and Tajiki). As a low-resource language, datasets for Tajiki are sparse, much less parallel Farsi-Tajiki datasets. Facing a lack of data, our data-acquisition strategy consisted of manual collection of blog posts written in both scripts. We then utilized GaChalign, an implementation of the Gale-Church sentence aligner (Gale & Church, 1993) to align the texts (Lilling & Francis, 2014). Our efforts created the first sentence-aligned, digraphic Persian corpus containing around 2,300 sentences and 66,000 words. Our preliminary investigations delivered promising results. Our current model correctly transliterates 39.2% of words from our test set of 3,507 words. After predictions that are one and two edit-distances away from the correct form, the accuracy increases to 66.7% and 82.2%, respectively. We evaluate our model using the BLEU score metric (Papineni et al., 2002), with the results presented in Figure 1. Our progress shows that a neural network-based G2P model is a viable method of transliteration between both dialects. We envision that the final form of the tool will consist of a freely-available browser extension.</details><hr><b>Student talk</b> <i>Seminar Room 23.21.00.048</i><br><details><summary>▼One Parent one language, one child two Grammars - <i>Sumrah Arshad</i></summary> [SLIDES]<br>This study investigates the acquisition of negation and NC in bilingual children acquiring Dutch and Italian simultaneously.\n\nZeijlstra(2004, et seq.) hypothesises that formal features of negation are not present in the language input and all the features of negation are semantic. Only the input can guide children to assume formal features of negation. Zeijlstra's hypothesis seems straightforward for Dutch which allows only one negative element per clause and is considered as double negation language. Dutch acquiring children receive excessive input for acquiring the adult-like expression of negation. This hypothesis also seems straightforward for children acquiring only Italian which allows the use of more than one negative element per clause and is called as negative concord language. Italian acquiring children also receive explicit input for the acquisition of negation of Italian. For the expression of negation Dutch and Italian belong to two distant groups. In this study we investigate data of young children acquiring Dutch and Italian simultaneously.\n\nHypothesis: Bilingual children acquiring a double negation language Dutch and an NC language Italian will have mixed grammars.\n\nNaturalistic spoken speech of children acquiring Italian and Dutch simultaneously is retrieved from CHILDES. Negative sentences for children and parents, per month, for each language, were searched. After cleaning, Italian sentences were then divided into containing i) the negative marker non as the only negative element, ii) sentences for negative concord. Dutch negative sentences contain mainly the negative marker niet.\n\nIn order to estimate the effect of independent variables on the dependent variable, a GLMM (Baayen, 2008) with a Poisson error structure and the log link function (McCullagh & Nelder, 1989) was used. Full null model comparison was significant. The main effect of individual fixed effects was tested using the drop1 function (Dobson, 2002).\n\nResults Dutch: Parents' input for niet (LRT: x2 (1)=6.38, p< 0.001) and age (LRT: x2 (4)=27.59, p< 0.0001) both show a positive significant effect on bilingual children's acquisition of negation. No sentence containing more than one negative element was found in children's or parents' sentences. Post hoc multiple comparisons show that children of different age-groups did not differ in observing the estimated average use of niet, thus suggesting that bilingual children acquire negation in Dutch very early, similar to their L1 peers.\n\nItalian: Non: Parents' input containing non only has a significant positive effect on children's non as the only negative marker (LRT:x2(1)=22.21, p< 0.0001). Age was also significant.\n\nNC: Parents' input for NC did not show a significant effect on children's acquisition of NC but age was significant (LRT: x2 (1) = 6.51, p = 0.01).\n\nNo sentence was found containing any of the Dutch negative elements. First NC was found in the 33rd month, and the first instance of NC in L1 Italian peers was found at the age of 23 months.\n\nConclusion: We conclude that bilingual children acquiring Dutch and Italian simultaneously as their L1s do not mix the grammar of both of their languages. Their acquisition of negation in Dutch and Italian is similar to their respective peers acquiring only one of the language.</details>"
- time: 12:40 - 13:40
  info: "<b>Lunch </b><i> University Mensa</i>"
- time: 13:40-14:10
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.46</i><br><details><summary>▼ BART meets Macbeth - Summarization of Shakespeare's plays with BART based models - <i>Rishu Kumar, Katja Konermann, Jingyan Chen</i></summary>\nClick <a href='/css/2023_style/TalkSlides/KUMAR.pptx'>here</a> to download the slides.\n<br>With the rise in multilingual models and their increasing capacity in multiple downstream tasks, this study explores a recent state-of-the-art model forthe downstream task of summarization. We explore different summarization techniques, namely abstractive and dialogue summarization. Dialogues pose a special challenge to the task of summarization as their format differs quite a bit from other types of text. Especially in plays, important actions and crucial pieces of information are often only stated indirectly or not in the dialogue itself but in short stage directions. Additionally, summaries of dialogues require a large amount of rewording as well as rephrasing. For example, instances of the pronoun in 1st person usually have to be replaced by a 3rd person pronoun. For this study, we worked with Shakespeare plays in their original English version and German translations of them. This process included creating our own dataset by manually mapping chunks of the plays to short summaries. The antiquated language in these plays presents a further challenge, mainly because it anslated into present-time English for the summaries. Our approach tests the language-agnostic vocabulary space of BART-based multilingual models by providing input in Old English, New English and German. We further fine-tune our models for dialogue summarization with the SAMSUM dataset in English and its machine-translated equivalent in German as well as our own hand-crafted datasets. In this talk, we will walk you through our data collection and dataset creation, discuss preprocessing steps as well as the training and evaluation of the different models. Finally, we will talk about extensions and further research in this area.</details><hr><b>Student talk</b> <i>Seminar Room 23.21.00.048</i><br><details><summary>▼ Lexicon-based data synthesis for Swiss German NLP - <i>Barabara Kovačić</i></summary>\nClick <a href='/css/2023_style/TalkSlides/KOVACIC.pdf'>here</a> to download the slides.\n<br>People communicate and present their opinion in their native languages, especially with regional expressions including dialects. Especially on Social Media, dialectal words and phrases can be found. With half of the human population having a social media account, the demand for NLP tools that can handle dialectal expressions has never been more important. But often, there is a lack of resources for dialectal language. One possible solution to data scarcity is to synthetically increase the available data - however, low-resource languages are a challenging application for data synthesis, as the existing methods are less beneficial when applied to out-of-domain data. Therefore, many pretrained models cannot be effectively used. This leads to the question, how data synthesis can be effectively used to improve language models handling data including dialectal expressions. The German language has the reputation of being a pluricentric language as it is the national language of six countries, Switzerland amongst them. Swiss German alone can be divided into six to eight different dialects. Therefore, there have been numerous annotation efforts that pay specific attention to Swiss German, such as ArchiMob, SwissDial and NOAH. When trying to use language models for Swiss German which were pretrained on a Standard German dataset, the same problems occur as previously mentioned although there are only minor differences in the grammar. As the major differences between these two varieties are on the phoneme level, it can be beneficial to enhance a Standard German dataset with Swiss German words so that the language model is more robust to their occurrences when working on a Swiss German dataset. Therefore, this bachelor thesis aims to augment the datasets in a way so that they can be used for existing, pretrained language models. In doing so, the focus will be on the task of part-of-speech tagging.</details>"  
- time: 14:20-14:50
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.046</i><br><details><summary>▼ Coronaleugnerchats auf Telegram - Ein distinktiver Schreibstil? - <i>Rebekka Borges</i></summary>\nClick <a href='/css/2023_style/TalkSlides/BORGES.pptx'>here</a> to download the slides. \n<br>Durch die Covid-19-Pandemie entstanden riesige Telegram-Gruppen, in denen Coronaskeptiker sich über Proteste und Verschwörungsmythen austauschen. Diese Kommunikation findet in einem sehr typischen Jargon statt. Ob sogar von einem distinktiven Coronaleugner-Stil gesprochen werden kann, den die Gruppenmitglieder bspw. zu identitätsbildenden Zwecken wählen, sollen verschiedene Klassifikationsexperimente zeigen. Dafür wurden Coronaleugnerchats zunächst mit einer Accuracy von 99 % in Abgrenzung gegen Telegram-Newstexte mit einer BERT-Implementation als solche klassifiziert. Da der Entscheidungsprozess neuronaler Netze eine Blackbox darstellst, ist nicht ersichtlich, nach welchen Kriterien die Klassifikation erfolgt, also welche Merkmale sich beim Training als wie relevant erweisen. Merkmale können über Stichworte hinausgehen und sehr abstrakt und wenig offensichtlich sein wie z. B. Wortsequenzen oder die Wortlänge. Ein Stil im variationslinguistischen Sinn umfasst neben lexikalischen Markern auch abstraktere Muster wie die hier womöglich relevanten. Die Experimente sollen zeigen, ob ein Neuronales Netz mit einer simpleren, auf Stichworte ausgerichtete Architektur generell sowie bei der Manipulation verschiedener Stichwortevorkommen schlechtere Ergebnisse erbringt und somit auf relevante abstrakte Merkmale wie die eines Stils geschlossen werden kann. Die verschiedenen Korpusversionen für die Experimente (alle beinhalten ungefähr 60.000 Samples mit einer maximalen Länge von 50 Wörtern) werden in zehn Subsets für Kreuzvalidierungen unterteilt und diese werden im Verhältnis 90:10 in Training- und Testset gesplittet. BERT performt mit ~ 99 % Accuracy und die optimale CNN-Architektur mit zwei convolutional layers mit maximal 91 %. Für die auf die Gruppen-Chats versus Nachrichtentexte folgenden Experimente wurden Stichwörter, die besonders oft in den korrekt klassifizierten Samples enthalten waren, maskiert. Zudem wurden Telegram-Nachrichten zum Thema Corona von Coronaleugner gegen das Bundesministerium für Gesundheit abgeglichen. Außerdem wurde der Effekt des Kommunikationssettings untersucht. Aus dem schlechter als BERT performenden CNN lässt sich ableiten, dass es mehr relevante Merkmale als nur Stichworte gibt, wie auch aus dem bei der Abgrenzung von Nachrichten zum selben Thema performenden CNN. Der Unterschied zwischen Gruppen- und Kanal-Nachrichten ist minimal und nicht signifikant. Wurden Emojis und Satzzeichen ebenfalls vektorisiert, konnte die Performance gehoben werden. Viele Ergebnisse deuten also darauf hin, dass ein gemeinsamer, über Stichworte hinausgehender Stil anzunehmen ist.</details><hr><b>Student talk</b> <i>Seminar Room 23.21.00.048</i><br><details><summary>▼ Language Revitalization - A case for Idu Mishmi  - <i>Akhilesh Kakolu Ramarao</i></summary>\nClick <a href='https://akkikek.xyz/presentations/tacos.html'>here</a> to download the slides. \n<br>Arunachal Pradesh is one of the linguistically richest and most diverse regions in all of Asia. In part due to this high diversity, a Hindi-based creole has been rapidly sweeping the state in recent years. The focus of this talk is around the on-going language revitalization efforts for Idu Mishmi language, a threatened language of Arunachal Pradesh. I will be sharing my experience of working with the Idu Mishmi community to develop technologies (like dictionary application, e-reader applications etc.) and learnings from the field.</details>"  
- time: 14:50 - 15:30
  info: "<b>Coffee Break </b><br><i>Seminar Room 23.21.00.48</i>"
- time: 15:30 - 16:30
  info: "<b>Keynote talk by Dr. Kilian Evang</b> <i>Lecture Hall 3F</i><br><details><summary>▼ Semantic Roles for Semantic Parsing Lessons Learned and Future Directions</summary>\nClick <a href='/css/2023_style/TalkSlides/EVANG.pdf'>here</a> to download the slides. \n<br>In this talk I report on efforts to annotate a parallel Role and Reference Grammar treebank (RRGparbank) with semantic roles for verbs. I discuss the annotation scheme developed, the annotation process, and semantic parsing results. Finally, I highlight some problems with existing annotation schemes based on VerbNet, PropBank, FrameNet, and VerbAtlas, and sketch a new scheme that solves some of these problems.</details>"
