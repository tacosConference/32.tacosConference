- time: 9:45 - 10:00
  info: "<b>Day 2 Overview</b> <i>Lecture Hall 3F</i><br>Information for the day"
- time: 10:00 - 11:00
  info: "<b>Keynote talk by Dr. Nurul Lubis</b> <i>Lecture Hall 3F</i><br><details><summary>▼ Dialogue Evaluation via Offline Reinforcement Learning and Emotion Prediction</summary> [SLIDES]<br>Task-oriented dialogue systems aim to fulfill user goals, such as booking hotels or searching for restaurants, through natural language interactions. They are ideally evaluated through interaction with human users. However, this is unattainable to do at every iteration of the development phase due to time and financial constraints. Therefore, researchers resort to static evaluation on dialogue corpora. Although they are more practical and easily reproducible, they do not fully reflect real performance of dialogue systems.\n\nCan we devise an evaluation that keeps the best of both worlds? In this talk I explore the usage of offline reinforcement learning and emotion prediction for dialogue evaluation that is practical, reliable, and strongly correlated with human judgements.</details>"
- time: 11:00-11:30
  info: "<b>Coffee Break</b> <i>23.21.00.44</i>"
- time: 11:30-12:00
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.46</i><br><details><summary>▼ Evaluation of Russian Noun Word Embeddings For Cases and a Number - <i>Anastasia Yablokova</i></summary>\nClick <a href='/css/2023_style/TalkSlides/YABLOKOVA.pdf'>here</a> to download the slides. \n<br>Russian can be characterized by a rich inflectional morphology. Particularly Russian nouns can illustrate this variety by changing the word form to indicate its grammatical case and number. The word embeddings for each of the case noun forms in singular and plural will be represented by its own real-valued vector of the length of 300 dimensions that encodes word meaning, so that words that are close in the embedding space should also be close in their meaning. Some other languages that have different noun forms for a number and/or case may exhibit interesting features in word embeddings. For example, Shafaei-Bajestan et al. (2022) have examined the semantic properties of English nominal pluralization and word embeddings and have found out that shift vectors for words, that belong to different groups on the basis of their meaning, are substantially different. This research has encouraged us to look closer at Russian noun embeddings and figure out if nouns that belong to the same group show any regularities on the basis of case forms and number. In order to do that, the dataset of 1700 nouns has been divided into groups on the basis of their semantical similarity, then enlarged with 12 columns that correspond to six Russian noun cases in two numbers (singular and plural). After that the fastText library is applied to get word vectors for each of the noun case forms. Then the difference vectors between base form (nominative case) and the other cases are calculated for each noun in the dataset. The average vectors are calculated for each group of words for every case form. The resulting vectors are added to be the base word form and then compared to the initial fastText vector.\n\nWe assume that words that belong to the same group may have the same average vector for every case form. The results will help us to understand word embeddings better, thus improve word representations</details><hr><b>Student talk</b> <i>Seminar Room 23.21.00.048</i><br><details><summary>▼ Exploring Song Topics Across Different Countries - A Latent Dirichlet Allocation Approach  - <i>Nursulu Sagimbayeva</i></summary> [SLIDES]<br>Empirically, it is well-known that many songs produced by humanity are devoted to love and feelings. However, it is interesting to explore what other topics are common in songs. A step further is to look at the song topic distribution across different countries. Could it be so that, for example, the topic 'money' will be more widespread in America than in other countries, while the topic 'Politics' will prevail in Russian charts?\n\nIn this project, we discover and compare the topics in popular songs of 20 different countries using LDA (Latent Dirichlet Allocation) topic modeling. Additionally, we analyze the most common languages of the songs popular in a given region.\n\nTo gather data, we used Spotify's weekly charts at Kworb.net since they captured historic data and not only songs popular on a given day or week. We scraped the lyrics of the top 200 songs for each country from Genius.com. Then, we translated all the lyrics into English and preprocessed them. In the end, we performed LDA using the gensim library and visualized the results with the pyLDAvis tool.\n\nOur results suggest that there are some topics popular in all the researched countries: different shades of love (for example, romantic, unhappy, sensual), and the so-called 'thug life' topic that consists of cursing words, mentions of money, drugs, and so on. However, the significance of each topic and its exact content vary from country to country.\n\nRelevance: The result of topic modeling on songs' lyrics can be used in cultural and comparative studies, comparative analysis of the countries, historical analysis of trends over time, marketing, but also just to give an insight into a certain audience's preference.</details>"  
- time: 12:10-12:40
  info: "<b>Student talk</b> <i>Seminar Room 23.21.00.46</i><br><details><summary>▼ Microsyntactic Unit Analysis using Word Embedding Models - Experiments on Slavic Languages - <i>Iuliia Zaitova, Irina Stenger, Tania Avgustinova</i></summary>\nClick <a href='/css/2023_style/TalkSlides/ZAITOVA.pdf'>here</a> to download the slides.\n<br>Microsyntactic units have been defined as language-specific transitional entities between lexicon and grammar, which idiomatic properties are closely tied to syntax. While these units are abundant and diverse, they are typically described based on individual constructions,making their comprehensive understanding difficult.\n\nThis study proposes a novel approach to detect microsyntacticunits using Word Embedding Models (WEMs) trained on six Slavic languages, namely, Belarusian, Ukrainian, Russian, Bulgarian, Czech, and Polish and evaluates how well these models capture the nuances of syntactic compositionality.\n\nTo address this challenge, we apply two different WEMs that previously proved effective at idiomaticity detection, namely Word2Vec CBOW and Context2Vec, as well as three adaptations of Word2Vec for syntactic tasks, namely Word2Vec CWINDOW, Word2Vec Structured, and Node2Vec. The training data is sourced primarily from the Leipzig Corpora Collection and the Russian National Corpus.\n\nTo evaluate the models, we develop a cross-lingual inventory of microsyntactic units using the lists of microsyntantic units available at the Russian National Corpus. We extracted 50 most frequent microsyntactic units from each category (prepositions, adverbial and predicatives, parenthetical expressions, conjunctions, and particles), resulting in parallel sets of 227 microsyntactic units with their context sentences, each for one of the six Slavic languages under analysis.\n\nOur results demonstrate the effectiveness of WEMs in capturing microsyntactic units and identifying their compositionality. We find that simple Word2Vec embedding models adapted for syntactic tasks perform best, even when compared to neural-based DSMs. We show that the behavior of WEMs is consistent across all six Slavic languages under analysis, validating our proposed approach as applicable and effective for identifying microsyntactic units.\n\nOur findings contribute to the theory of microsyntax by providing insights into the detection of microsyntactic units and their crosslinguistic properties. Our approach has practical applications in natural language processing, machine translation, and computational linguistics, where the identification of microsyntactic units can improve the accuracy of tasks such as syntactic parsing and named entity recognition.</details><hr><b>Student talk</b> <i>Seminar Room 23.21.00.048</i><br><details><summary>▼ How do You measure Style (And Much More) - <i>Mikhail Sonkin</i></summary>\nClick <a href='/css/2023_style/TalkSlides/SONKIN.pdf'>here</a> to download the slides.\n<br>If you were to give ten English scholars two English texts and ask which one was written by Jane Austen and which – by Charlotte Brontë, they would most probably have no difficulty in answering correctly. However, if you were to ask them to explain the motivation behind their answer, their responses would most definitely differ from each other.\n\nThis little thought experiment begs the question: how do you automate that task? What exactly do you give to a computer to make it understand that two texts are written by different authors? Moreover, how do you make your algorithm not depend on a particular language? This is the problem of automatic authorship attribution.\n\nTo resolve this, many scholars have tried to involve statistical methods. Only one rather simple method, however, seemed to stick – Burrow's Delta. Invented by John Burrows in 2002, the Delta Analysis has proved in time to be a robust instrument for  authorship attribution. To identify, whether a document was written by Author A or Author B, you would need to collect a corpus of written texts and see, which author's “style” the document is most similar to.\n\nOf course, the scope of stylometry goes beyond authorship: many methods have been derived to compare different stylistic qualities of two authors. In that context, we will discuss one instrument in particular: the Zeta Analysis, which compares two corpora by extracting their keywords.\n\nIn this talk, we will look “under the hood” of the two methods and try to understand how exactly they succeed in their tasks, what their advantages and drawbacks are.\n\nWe will also discuss several cases in which these instruments serve a different function, such as: Assessing the quality of a parody, finding differences in characters' speech in dramatic works based on gender and family relation, detecting the translator's influence on a literary text. Come join and learn about how Digital Humanities deals with the intricacies of stylometry!</details>"
- time: 12:40 - 13:30
  info: "<b>Lunch</b> <i>University Mensa</i>"
- time: 13:30-15:30
  info: "<b>Career Networking Meet-up</b> <i>Seminar Room 23.21.00.44 </i><br>Opportunity to discover future career paths and learn from people in the industry and academia. Sponsors from industry and academics from HHU will be there to tell you about their careers in computational linguistics and answers questions about yours in an informal atmosphere. (There will be coffee)" 
- time: 15:30 - 16:30
  info: "<b>Keynote talk by Apl.Prof Wiebke Petersen</b> <i>Lecture Hall 3F</i><br><details><summary>▼ On representation techniques in Panini's grammar of Sanskrit - Solving an ancient problem</summary>\nClick <a href='/css/2023_style/TalkSlides/WIEBKE.pdf'>here</a> to download the slides. \n<br>Panini's grammar of Sanskrit is one of the oldest recorded grammars (~350 BC), that has earned universal admiration among linguists: 'The descriptive grammar of Sanskrit, which Pānini brought to its perfection,  is one of the greatest monuments of human intelligence and an indispensable model for the description of languages' (Bloomfield 1929). Being a grammar designed for an oral tradition it uses representation techniques that aim at compactness, e.g., a semi-formalized meta-language and an intricate system of conventions governing rule applications.\nIn the talk I will introduce some of the techniques and focus on Panini's representation of phonological classes as intervals of a list. Already early commentators have asked whether this list is optimal with respect to length. I will show how Formal Concept Analysis can answer this question and why it is worth to know this analysis technique.</details>"
- time: 16:30 - Open
  info: "<b>Boardgames</b>"